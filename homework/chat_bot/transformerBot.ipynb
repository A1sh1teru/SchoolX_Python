{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ чат-бота: Hi. I'm sorry, but I don't know what to do.\n",
      "\n",
      "I'm not sure what I should do, and I can't do anything. But I know that I have to. And I want to be there for you. So I'll be here. You're going to have a great time. Thank you for coming.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ чат-бота: Can you talk to me?\n",
      "\n",
      "I'm not sure. I'm just curious.\n",
      "...\n",
      " (pause)\n",
      "You're not going to tell me anything. You're going not to say anything, are you? I don't know. It's just that I've never been able to talk with you. And I know you're a little bit of a stranger. But I can't tell you anything about your life. So I'll just say that you've been a good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Programming\\schoolx_python\\homework\\chat_bot\\transformerBot.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming/schoolx_python/homework/chat_bot/transformerBot.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming/schoolx_python/homework/chat_bot/transformerBot.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mВаш вопрос: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Programming/schoolx_python/homework/chat_bot/transformerBot.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     response \u001b[39m=\u001b[39m generate_response(user_input)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Programming/schoolx_python/homework/chat_bot/transformerBot.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mОтвет чат-бота:\u001b[39m\u001b[39m\"\u001b[39m, response)\n",
      "\u001b[1;32md:\\Programming\\schoolx_python\\homework\\chat_bot\\transformerBot.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming/schoolx_python/homework/chat_bot/transformerBot.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_response\u001b[39m(input_text):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming/schoolx_python/homework/chat_bot/transformerBot.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(input_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Programming/schoolx_python/homework/chat_bot/transformerBot.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     response \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39minput\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, top_k\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming/schoolx_python/homework/chat_bot/transformerBot.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     response_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(response[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Programming/schoolx_python/homework/chat_bot/transformerBot.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response_text\n",
      "File \u001b[1;32mc:\\Users\\velic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\velic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1583\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[39m# decoder-only models should use left-padding for generation\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n\u001b[0;32m   1578\u001b[0m     \u001b[39m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[0;32m   1579\u001b[0m     \u001b[39m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n\u001b[0;32m   1580\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1581\u001b[0m         generation_config\u001b[39m.\u001b[39mpad_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1582\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(inputs_tensor\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m-> 1583\u001b[0m         \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39msum(inputs_tensor[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39m generation_config\u001b[39m.\u001b[39mpad_token_id) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1584\u001b[0m     ):\n\u001b[0;32m   1585\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1586\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1587\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1588\u001b[0m         )\n\u001b[0;32m   1590\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1591\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\" \n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate_response(input_text):\n",
    "    input = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    response = model.generate(input, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)\n",
    "    response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    return response_text\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Ваш вопрос: \")\n",
    "    response = generate_response(user_input)\n",
    "    print(\"Ответ чат-бота:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
