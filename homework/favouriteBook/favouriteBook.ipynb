{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pip setuptools wheel\n",
    "%pip install -U spacy\n",
    "%pip install -q spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.tokenize\n",
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import NLTKWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from nltk.tokenize import SExprTokenizer\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.tokenize import TabTokenizer\n",
    "from nltk.tokenize import LineTokenizer\n",
    "from nltk.tokenize import LegalitySyllableTokenizer\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "with open('./jules-verne_mysterious-island.txt', 'r', encoding='utf-8') as file:\n",
    "    bookText = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "\n",
    "### Выполнить токенизацию текста любимой книги с использованием токенизаторов библиотек nltk и spacy. Описать отличия, достоинства и недостатки токенизаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordPunctTokenizer**\n",
    "\n",
    "*Преобразует текст в последовательность буквенных и неалфавитных символов с помощью регулярного выражения ‘\\w+|[^\\w\\s]+’. Отличается от WhitespaceTokenizer тем, что отделяет знаки препинания и иные символы пунктуации от слова, за / перед которым они идут, и возвращает их в качестве отдельных токенов.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPunct = WordPunctTokenizer()\n",
    "print(wordPunct.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WhitespaceTokenizer**\n",
    "\n",
    "*При помощи этого токенизатора исходный текст разделяется на группы символов, ограниченных пробелами (обычно, это отдельные слова, либо слова со следующими за ними знаками препинания).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace = WhitespaceTokenizer()\n",
    "print(whitespace.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTKWordTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTok = NLTKWordTokenizer()\n",
    "print(wordTok.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RegexpTokenizer**\n",
    "\n",
    "*RegexpTokenizer имеет 4 параметра:*\n",
    "\n",
    "*pattern – шаблон регулярного выражения, используемый для создания токенизатора;*\n",
    "\n",
    "*gaps – логический параметр, определяющий, будет ли токенизатор искать сами токены (False) или разделители между ними (True). В первом случае результат работы токенизатора для указанного примера будет аналогичен выполнению команды re.findall(pattern, text) с использованием библиотеки re;*\n",
    "\n",
    "*discard_empty – необходимо установить в значение True, если необходимо отбросить пустые токены, сгенерированные токенизатором (возможно только в случае gaps == True);*\n",
    "\n",
    "*flags – флаги регулярного выражения, используемы при компиляции шаблона. По умолчанию установлены 3 флага: re.UNICODE | re.MULTILINE | re.DOTALL.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '[bookText]\\w+'\n",
    "regexpTok = RegexpTokenizer(pattern)\n",
    "print(regexpTok.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BlanklineTokenizer**\n",
    "\n",
    "*Как понятно из названия, разбивает текст, рассматривая в качестве разделителя любую последовательность пустых строк (содержащих только символы пробела или табуляции). Для этого используется регулярное выражение ‘\\s^\\n\\s^\\n\\s^’.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanklineTok = BlanklineTokenizer()\n",
    "print(blanklineTok.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SExprTokenizer**\n",
    "\n",
    "*Этот токенизатор используется для поиска в исходном тексте выражений в скобках. В частности, он делит строку на последовательность подстрок, являющихся выражениями в скобках (включая любые вложенные выражения в скобках) или другими токенами, разделенными пробелами.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookTextExample = ('(x + y / (14 + z)) * a - b) + 34 / (87')\n",
    "sexprTok = SExprTokenizer(strict=False)\n",
    "print(len(sexprTok.tokenize(bookText)))\n",
    "print(sexprTok.tokenize(bookTextExample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SpaceTokenizer**\n",
    "\n",
    "*Разбивает строку на токены, используя в качестве разделителя пробел. Результат работы этого метода полностью совпадает с результатом метода s.split(‘ ‘).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceTok = SpaceTokenizer()\n",
    "print(spaceTok.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TabTokenizer**\n",
    "\n",
    "*Аналогичен SpaceTokenizer, только для разделения строки использует символ табуляции, что совпадает с применением метода s.split(‘\\t’).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabTok = TabTokenizer()\n",
    "print(tabTok.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LineTokenizer**\n",
    "\n",
    "*Аналогичен SpaceTokenizer, за исключением того, что для разделения строки использует символ новой строки (при необходимости может отбрасывать пустые строки). Его применение похоже на применение метода s.split(‘\\n’).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineTok = LineTokenizer()\n",
    "print(lineTok.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LegalitySyllableTokenizer**\n",
    "\n",
    "*Разбивает слова по слогам, основываясь на принципе правильности и максимизации наборов.*\n",
    "\n",
    "*Принцип правильности (Legality Principle) – это не зависящий от используемого языка принцип, утверждающий, что начала и окончания слогов (не включая гласный звук) допустимы только в том случае, если они встречаются в качестве начал или окончаний слов в языке.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SyllableTokenizer**\n",
    "\n",
    "*Это еще один токенизатор для разбиения слов на слоги. Его работа основана на принципе последовательности звучания (Sonority Sequencing Principle, SSP).*\n",
    "\n",
    "*Алгоритм SSP так же, как и алгоритм Legality Principle не зависит от используемого языка.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MWETokenizer (Multi-Word Expression Tokenizer)**\n",
    "\n",
    "*MWETokenizer берет строку, которая уже была разделена на токены, и повторно токенизирует ее, объединяя выражения из нескольких слов в отдельные токены, используя словарь MWE.*\n",
    "\n",
    "*Это может быть полезно в том случае, когда в тексте встречаются устойчивые выражения или именованные сущности, состоящие из нескольких слов.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextTilingTokenizer**\n",
    "\n",
    "*TextTiling – это метод автоматического разделения полноразмерных текстовых документов на последовательные блоки, состоящие из нескольких абзацев, которые представляют собой отрывки или подтемы.*\n",
    "\n",
    "*Алгоритм предполагает, что в ходе описания подтемы используется определенный набор слов, и когда подтема изменяется, значительная часть словарного запаса также изменяется.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToktokTokenizer**\n",
    "\n",
    "*Токенизатор Toktok – это простой универсальный токенизатор, на вход которого подается по одному предложению в строке*\n",
    "\n",
    "*Работа токенизатора основана на последовательном применении к исходному тексту списка регулярных выражений. С их помощью происходят, например, замена неразрывных пробелов обычными, замена знака многоточия единичной точкой, расстановка пробелов после открывающих и перед закрывающими знаками препинания. В итоговой строке удаляются начальные и конечные пробелы, а сама строка преобразуется в кодировку Unicode.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toktok = ToktokTokenizer()\n",
    "print(toktok.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TweetTokenizer**\n",
    "\n",
    "*Это токенизатор с поддержкой особенностей коротких сообщений Twitter, разработанный для гибкой и простой адаптации к новым предметным областям и задачам. Основная логика его работы заключается в следующем:*\n",
    "\n",
    "*   Определяется список регулярных выражений, для поиска в исходном тексте гиперссылок, смайликов, телефонных номеров, имен пользователей Twitter, хештегов и др\n",
    "\n",
    "*   Регулярные выражения помещаются по порядку в скомпилированный объект регулярного выражения, называемый word_re.\n",
    "\n",
    "*   Выполняется токенизация с помощью метода word_re.findall(s), где s – строка, исходного текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetTok = TweetTokenizer()\n",
    "print(tweetTok.tokenize(bookText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TreebankWordTokenizer**\n",
    "\n",
    "*Токенизатор Treebank использует регулярные выражения для разделения текста на слова так, как это реализовано в Penn Treebank. Для этого последовательно выполняются следующие шаги:*\n",
    "\n",
    "*   разделяются стандартные сокращения, например, «don’t» -> »do n’t» или «they’ll» -> «they ‘ll» для английского языка;\n",
    "*   большинство знаков препинания, разделяются на отдельные токены;\n",
    "*   разделяются запятые и одинарные кавычки, если за ними следует пробел;\n",
    "*   отделяются точки, которые проставлены в конце строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebankTok = TreebankWordTokenizer()\n",
    "print(list(treebankTok.tokenize(bookText)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StanfordSegmenter**\n",
    "\n",
    "*Некоторые языки требуют более обширной предварительной обработки токенов, которая обычно называется сегментацией. Например, StanfordSegmenter предназначен для «токенизации» или «сегментации» слов китайского или арабского текста (также можно использовать для английского, французского и испанского языков).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PunktSentenceTokenizer**\n",
    "\n",
    "*Этот токенизатор разделяет текст на список предложений, используя алгоритм обучения без учителя (Unsupervised learning) для построения модели слов-сокращений, словосочетаний и слов, с которых начинаются предложения.*\n",
    "\n",
    "*Прежде чем использовать, его необходимо обучить на большом наборе текстов, написанных на целевом языке.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy**\n",
    "\n",
    "*Библиотека для NLP, которая предлагает высокопроизводительное решение для многих задач обработки естественного языка, таких как частеречная разметка, именованные сущности, связывание слов и векторное представление текста.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_sm')\n",
    "doc = model(bookText)\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Отличия NLTK и SpaCy**\n",
    "\n",
    "*Токенизаторы из библиотеки NLTK основаны на правилах и регулярных выражениях. NLTK предоставляет гибкость в настройке токенизации, что позволяет создать пользовательские правила или настроить существующие. Однако, из-за этой гибкости, токенизатор NLTK может быть менее точным в некоторых случаях, особенно при обработке сложных языковых конструкций.*\n",
    "\n",
    "*Токенизаторы из библиотеки SpaCy, напротив, основаны на статистическом подходе и предварительно обучены на больших объемах текстов. В результате, они имеют достаточно высокую точность и производительность. SpaCy также предоставляет широкий набор инструментов для обработки текста на естественном языке. Однако, токенизаторы spaCy могут быть менее гибкими и не позволяют внести пользовательские настройки.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "### Определить лексическое разнообразие книги (TTR).\n",
    "\n",
    "*TTR - это отношение количества уникальных слов к общему количеству слов в тексте.* \n",
    "\n",
    "*Чем выше значение TTR, тем больше разнообразие слов в тексте.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(bookText):\n",
    "    words = re.findall(r'\\w+', bookText)\n",
    "    return words\n",
    "\n",
    "print('Список всех найденных слов в тексте:')\n",
    "tokens = word_tokenizer(bookText)\n",
    "print(tokens)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Количество всех найденных слов в тексте:')\n",
    "wordsCount = len(tokens)\n",
    "print(wordsCount)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Список всех уникальных слов в тексте:')\n",
    "uniqueWords = list(set(tokens))\n",
    "print(uniqueWords)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Количество всех уникальных слов в тексте:')\n",
    "uniqueWordsCount = len(uniqueWords)\n",
    "print(uniqueWordsCount)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Тип-токен-отношение(TTR):')\n",
    "ttr = uniqueWordsCount/wordsCount\n",
    "print(ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "\n",
    "### Написать собственный токенизатор, выполненный в качестве метода класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Мой токенизатор разделяет текст на слова и знаки препинания, а также, по моей собственной задумке, преобразует все слова в числа (число равно количеству букв в слове).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Регулярное выражение \\b\\w+\\b|\\S соответствует группам одного или более символов слова, окруженных границами слов (\\b\\w+\\b) или любым непробельным символом (\\S), который представляет собой знаки пунктуации.* \n",
    "\n",
    "*Так как для обработки я брал текст из книги на английском языке, то для каждого токена проверяется, является ли он словом на английском языке (предполагая, что только английские слова будут ASCII-допустимы), и затем заменяется числом, которое равняется количеству букв в слове.*\n",
    "\n",
    "*Примечание: числа, которые изначально были в тексте я не изменял.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ссылки на материалы, на которые я опирался при выполнении задания:*\n",
    "\n",
    "*   https://docs-python.ru/standart-library/modul-re-python/sintaksis-reguljarnogo-vyrazhenija/\n",
    "*   https://www.tutorialspoint.com/python/string_isalpha.htm\n",
    "*   https://docs-python.ru/tutorial/operatsii-tekstovymi-strokami-str-python/metod-str-isascii/\n",
    "*   https://docs-python.ru/standart-library/modul-re-python/funktsija-findall-modulja-re/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('./jules-verne_mysterious-island.txt', 'r', encoding='utf-8') as file:\n",
    "    bookText = file.read()\n",
    "\n",
    "class myOwnTokenizerWordsIntoNumbers:\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Разделение текста на слова и знаки препинания\n",
    "        tokens = re.findall(r'\\b\\w+\\b|\\S', text)\n",
    "        \n",
    "        # Замена слов на числа\n",
    "        tokens = [str(len(token)) if token.isalpha() and token.encode().isascii() else token for token in tokens]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "tokenizer = myOwnTokenizerWordsIntoNumbers()\n",
    "print(tokenizer.tokenize(bookText))\n",
    "\n",
    "# https://docs-python.ru/standart-library/modul-re-python/sintaksis-reguljarnogo-vyrazhenija/\n",
    "# https://www.tutorialspoint.com/python/string_isalpha.htm\n",
    "# https://docs-python.ru/tutorial/operatsii-tekstovymi-strokami-str-python/metod-str-isascii/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
